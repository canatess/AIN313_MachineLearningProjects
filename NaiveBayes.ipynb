{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxJQg6jeGTST"
      },
      "source": [
        "# ***AIN 313 - Assignment 2***\n",
        "\n",
        "### ***Instructor:*** Erkut Erdem\n",
        "### ***Assistant:*** Sibel Kapan\n",
        "### ***Topic:*** Naive Bayes\n",
        "### ***Subject:*** Book Genre Classification with Naive Bayes\n",
        "### ***Student Info:*** Can Ali Ateş\n",
        "### ***Student ID:*** 2200765002"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oliRV2sMw21R"
      },
      "source": [
        "# ***PART I - Theory Questions***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIIFYfpQtnLF"
      },
      "source": [
        "## ***Maximum Likelihood Estimation (MLE)***\n",
        "\n",
        "![Question1](https://drive.google.com/uc?id=1VQ27utuzMXbF0yU3s6jbOZmuQFUpeCq3)\n",
        "\n",
        "### ***Answer 1:***\n",
        "$Let's$ $say$ $P(X=Head) = k$ <br> <br>\n",
        "$Likelihood(k) = k.(1-k).k.(1-k).k = (1-k)^2.k^3$<br> <br>\n",
        "***Maximum Likelihood and Maximum Log Likelihood is equal because Log operation is monotonic transformation.*** <br> <br>\n",
        "$log(Likelihood(k)) = log((1-k)^2.k^3) = 2.log(1-k) + 3.log(k)$ <br> <br>\n",
        "***Derivative has to be 0 for maximization.*** <br><br>\n",
        "$\\frac{d}{d_k}log(Likelihood(k)) = 2.\\frac{1}{1-k}.(-1) + 3.\\frac{1}{k} =0\\;\\;\\;so \\;\\;\\; k_{estimated} =\\frac{3}{5}$ <br><br>\n",
        "$MLE(k) = \\frac{3}{5}$\n",
        "\n",
        "<br> <br>\n",
        "\n",
        "![Question2](https://drive.google.com/uc?id=1do2mG9tCEgCqbpbM8094AIM4FbFRRrcW)\n",
        "\n",
        "### ***Answer 2:***\n",
        "***Maximum Likelihood for mean in normal distribution is equal to mean of the sample directly.*** <br> <br>\n",
        "$MLE(µ) = \\frac{1}{N}\\sum_{1}^{N}X_i = \\frac{1}{3}.(2 + 3 + 5) = \\frac{10}{3}$\n",
        "\n",
        "<br> <br>\n",
        "\n",
        "![Question3](https://drive.google.com/uc?id=178e3tRjTWbP724p3FnogAF44GKNsHSS4)\n",
        "\n",
        "### ***Answer 3:***\n",
        "$Likelihood(θ) = P(x=3).P(x=0).P(x=2).P(x=1).P(x=3).P(x=2).P(x=1).P(x=0).P(x=2).P(x=1)$<br> <br>\n",
        "***Maximum Likelihood and Maximum Log Likelihood is equal because Log operation is monotonic transformation.*** <br> <br>\n",
        "$log(Likelihood(θ)) = \\sum_{i=1}^{n}logP(X_i|θ) = 2(log\\frac{2}{3}+logθ)+3(log\\frac{1}{3} + logθ)+3(log\\frac{2}{3}+log(1-θ))+2(log\\frac{1}{3}+log(1-θ))$ <br><br>\n",
        "$l(θ) = log(Likelihood(θ)) = 5logθ + 5log(1-θ) + C$ <br> <br>\n",
        "$\\frac{d}{dθ}log(Likelihood(θ)) = \\frac{5}{θ} - \\frac{5}{1-θ} = 0 \\;\\;\\; so \\;\\;\\; θ_{estimated} = 0.5$ <br> <br>\n",
        "$MLE(θ) = 0.5$\n",
        "\n",
        "<br>\n",
        "\n",
        "![Question4](https://drive.google.com/uc?id=10adBKY3XgtmLizPMpCDl696f_yHx3MWX)\n",
        "\n",
        "### ***Answer 4:***\n",
        "$f(x|θ) = 1/θ \\;\\;\\; if \\;\\;\\; 0≤x≤θ,\\;\\; otherwise\\;\\; 0$ <br> <br>\n",
        "$Likelihood(θ) = \\frac{1}{θ^n} \\;\\;\\; if \\;\\;\\; 0≤x_i≤θ \\;\\;\\; for \\;\\;\\; (i = 1,...,n), \\;\\;\\; otherwise \\;\\; 0$ <br> <br>\n",
        "***$\\frac{1}{θ^n}$ is a decreasing function of $θ$, so the $MLE = θ_{estimated} = max(X_1, ... , X_n)$***\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "## ***Naive Bayes***\n",
        "\n",
        "![Question5](https://drive.google.com/uc?id=1Aoja8E7COlHxae3oaBIokDhOGyW3-Zau)\n",
        "\n",
        "### ***Answer 1:***\n",
        "***We Need to Apply Laplace Smoothing because $P(A=16-25|C=y) = 0.0$*** <br> <br>\n",
        "$P(C=y) = \\frac{5}{10}$ <br><br>\n",
        "$P(C=n) = \\frac{5}{10}$ <br><br> \n",
        "$P(S=m|C=y) = \\frac{3+1}{5+4} = \\frac{4}{9}$ <br><br> \n",
        "$P(S=m|C=n) = \\frac{1+1}{5+3} = \\frac{2}{9}$ <br><br>\n",
        "$P(E=u|C=y) = \\frac{2+1}{5+4} = \\frac{3}{9}$ <br><br>\n",
        "$P(E=u|C=n) = \\frac{2+1}{5+4} = \\frac{3}{9}$ <br><br>\n",
        "$P(A=16-25|C=y) = \\frac{0+1}{5+4} = \\frac{1}{9}$ <br><br>\n",
        "$P(A=16-25|C=n) = \\frac{3+1}{5+4} = \\frac{4}{9}$ <br><br>\n",
        "$P(I=working|C=y) = \\frac{1+1}{5+4} = \\frac{2}{9}$ <br><br>\n",
        "$P(I=working|C=n) = \\frac{2+1}{5+4} = \\frac{3}{9}$ <br> <br>\n",
        "$P(C=y).P(S=m|C=y).P(E=u|C=y).P(A=16-25|C=y).P(I=w|C=y) = 0.0018$ <br><br>\n",
        "$P(C=n).P(S=m|C=n).P(E=u|C=n).P(A=16-25|C=n).P(I=w|C=n) = 0.0055$ <br><br>\n",
        "\n",
        "***0.0055 $\\ge$ 0.0018, so Credit Label = no.*** <br>\n",
        "***As a result, The male 23-year-old university graduate working class customer can't get credit.***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBiw4mDzw4t0"
      },
      "source": [
        "# ***PART II -  Book Genre Classification with Naïve Bayes***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De9vYmltTDeb"
      },
      "source": [
        "***Import Libraries***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kUL-xwOw9Pk"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries.\n",
        "import math\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYsFvJAITJsM"
      },
      "source": [
        "***Read the Data***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "9AQHQiZaG-Bk",
        "outputId": "bd17ea26-6946-4f02-a50b-df9d6a904763",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>description</th>\n",
              "      <th>coverImg</th>\n",
              "      <th>genre</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Hunger Games</td>\n",
              "      <td>Suzanne Collins</td>\n",
              "      <td>WINNING MEANS FAME AND FORTUNE.LOSING MEANS CE...</td>\n",
              "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
              "      <td>Young Adult</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Harry Potter and the Order of the Phoenix</td>\n",
              "      <td>J.K. Rowling, Mary GrandPré (Illustrator)</td>\n",
              "      <td>There is a door at the end of a silent corrido...</td>\n",
              "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
              "      <td>Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>To Kill a Mockingbird</td>\n",
              "      <td>Harper Lee</td>\n",
              "      <td>The unforgettable novel of a childhood in a sl...</td>\n",
              "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
              "      <td>Classics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pride and Prejudice</td>\n",
              "      <td>Jane Austen, Anna Quindlen (Introduction)</td>\n",
              "      <td>Alternate cover edition of ISBN 9780679783268S...</td>\n",
              "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
              "      <td>Classics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Twilight</td>\n",
              "      <td>Stephenie Meyer</td>\n",
              "      <td>About three things I was absolutely positive.\\...</td>\n",
              "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
              "      <td>Young Adult</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       title  \\\n",
              "0                           The Hunger Games   \n",
              "1  Harry Potter and the Order of the Phoenix   \n",
              "2                      To Kill a Mockingbird   \n",
              "3                        Pride and Prejudice   \n",
              "4                                   Twilight   \n",
              "\n",
              "                                      author  \\\n",
              "0                            Suzanne Collins   \n",
              "1  J.K. Rowling, Mary GrandPré (Illustrator)   \n",
              "2                                 Harper Lee   \n",
              "3  Jane Austen, Anna Quindlen (Introduction)   \n",
              "4                            Stephenie Meyer   \n",
              "\n",
              "                                         description  \\\n",
              "0  WINNING MEANS FAME AND FORTUNE.LOSING MEANS CE...   \n",
              "1  There is a door at the end of a silent corrido...   \n",
              "2  The unforgettable novel of a childhood in a sl...   \n",
              "3  Alternate cover edition of ISBN 9780679783268S...   \n",
              "4  About three things I was absolutely positive.\\...   \n",
              "\n",
              "                                            coverImg        genre  \n",
              "0  https://i.gr-assets.com/images/S/compressed.ph...  Young Adult  \n",
              "1  https://i.gr-assets.com/images/S/compressed.ph...      Fantasy  \n",
              "2  https://i.gr-assets.com/images/S/compressed.ph...     Classics  \n",
              "3  https://i.gr-assets.com/images/S/compressed.ph...     Classics  \n",
              "4  https://i.gr-assets.com/images/S/compressed.ph...  Young Adult  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Update G-Down to avoid possible errors.\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "# Download the dataset from Google Drive to Google Colab.\n",
        "!gdown --id 1ZbB40qw7BU24q3G6gEMGD5kGMlJg33es\n",
        "\n",
        "# Read CSV file.\n",
        "df = pd.read_csv(\"book_dataset_a2.csv\", sep = \"\\t\")\n",
        "\n",
        "# Display first 5 row of DataFrame.\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPMGEfrNTVnt"
      },
      "source": [
        "***Preprocess the Data***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2l-l6Y6AHV2p",
        "outputId": "b59a01ee-bf92-44ca-8e29-4148c1b65672"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>description</th>\n",
              "      <th>coverImg</th>\n",
              "      <th>genre</th>\n",
              "      <th>cleaned_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>winning means fame and fortune losing means ce...</td>\n",
              "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
              "      <td>Young Adult</td>\n",
              "      <td>winning means fame fortune losing means certai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>there is door at the end of silent corridor an...</td>\n",
              "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
              "      <td>Fantasy</td>\n",
              "      <td>door end silent corridor haunting harry pottte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the unforgettable novel of childhood in sleepy...</td>\n",
              "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
              "      <td>Classics</td>\n",
              "      <td>unforgettable novel childhood sleepy southern ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>alternate cover edition of isbn since its imme...</td>\n",
              "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
              "      <td>Classics</td>\n",
              "      <td>alternate cover edition isbn immediate success...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>about three things was absolutely positive fir...</td>\n",
              "      <td>https://i.gr-assets.com/images/S/compressed.ph...</td>\n",
              "      <td>Young Adult</td>\n",
              "      <td>things absolutely positive edward vampire seco...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         description  \\\n",
              "0  winning means fame and fortune losing means ce...   \n",
              "1  there is door at the end of silent corridor an...   \n",
              "2  the unforgettable novel of childhood in sleepy...   \n",
              "3  alternate cover edition of isbn since its imme...   \n",
              "4  about three things was absolutely positive fir...   \n",
              "\n",
              "                                            coverImg        genre  \\\n",
              "0  https://i.gr-assets.com/images/S/compressed.ph...  Young Adult   \n",
              "1  https://i.gr-assets.com/images/S/compressed.ph...      Fantasy   \n",
              "2  https://i.gr-assets.com/images/S/compressed.ph...     Classics   \n",
              "3  https://i.gr-assets.com/images/S/compressed.ph...     Classics   \n",
              "4  https://i.gr-assets.com/images/S/compressed.ph...  Young Adult   \n",
              "\n",
              "                                 cleaned_description  \n",
              "0  winning means fame fortune losing means certai...  \n",
              "1  door end silent corridor haunting harry pottte...  \n",
              "2  unforgettable novel childhood sleepy southern ...  \n",
              "3  alternate cover edition isbn immediate success...  \n",
              "4  things absolutely positive edward vampire seco...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Drop title and author column.\n",
        "df = df.drop(['title', 'author'], axis = 1)\n",
        "\n",
        "# Convert all characters to lowercase.\n",
        "df['description'] = df['description'].str.lower()\n",
        "\n",
        "# Remove punctuations.\n",
        "df['description'] = df['description'].str.replace('[^\\w\\s]+', ' ', regex = True)\n",
        "\n",
        "# Remove numbers.\n",
        "df['description'] = df['description'].str.replace('\\d+', ' ', regex = True)\n",
        "\n",
        "# Remove non-English words.\n",
        "df['description'] = df['description'].str.replace('[^a-zA-Z\\s]+', '', regex = True)\n",
        "\n",
        "# Convert ENGLISH_STOP_WORDS set to tuple.\n",
        "stop_words = tuple(frozenset(ENGLISH_STOP_WORDS))\n",
        "\n",
        "# Remove the ENGLISH_STOP_WORDS from the descriptions, then save cleaned descriptions to new column. \n",
        "df['cleaned_description'] = df['description'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "\n",
        "# Remove single chars.\n",
        "df['description'] = df['description'].str.replace(' \\w ', ' ', regex = True)\n",
        "df['cleaned_description'] = df['cleaned_description'].str.replace(' \\w ', ' ', regex = True)\n",
        "\n",
        "# Remove multiple spaces, tabs and newlines.\n",
        "df['description'] = df['description'].str.replace('\\s+', ' ', regex = True)\n",
        "df['cleaned_description'] = df['cleaned_description'].str.replace('\\s+', ' ', regex = True)\n",
        "\n",
        "# Display the first 5 record of DataFrame.\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgp1osZVYHwT"
      },
      "source": [
        "### ***2.1 Understanding the Data***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "p8zVQPHKYHR2",
        "outputId": "4fdfbc22-8a4e-415b-c478-e5c4ce546dc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Most Frequent 3 Words in History Genre\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>history</td>\n",
              "      <td>776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>war</td>\n",
              "      <td>603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>world</td>\n",
              "      <td>512</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Word  Frequency\n",
              "0  history        776\n",
              "1      war        603\n",
              "2    world        512"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Group the genres, then choose History genre.\n",
        "history_books = df.groupby(['genre']).get_group('History')\n",
        "\n",
        "# Calculate the word frequencies in History genre.\n",
        "history_books_df = history_books.cleaned_description.str.split(expand=True).stack().value_counts().reset_index()\n",
        "\n",
        "# Arrange column names. \n",
        "history_books_df.columns = ['Word', 'Frequency'] \n",
        "\n",
        "# Display most frequent 3 words in History genre.\n",
        "print(\"Most Frequent 3 Words in History Genre\")\n",
        "history_books_df.iloc[:3, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFLLGexMmJlF"
      },
      "source": [
        "***Split the dataset into train and test sets.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQFUz59tmJlF"
      },
      "outputs": [],
      "source": [
        "# Feature and label splitting.\n",
        "X = df['description'].values   # We will use this for stopword experiments.\n",
        "cleaned_X = df['cleaned_description'].values    # We will use this for no-stopword experiments.\n",
        "y = df['genre'].values    \n",
        "\n",
        "# Encode the labels.\n",
        "encoder = LabelEncoder()\n",
        "encoded_y = encoder.fit_transform(y)\n",
        "\n",
        "# Train(%80) and Test(%20) split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, encoded_y, test_size=0.2, random_state = 42, stratify = encoded_y)\n",
        "cleaned_X_train, cleaned_X_test, cleaned_y_train, cleaned_y_test = train_test_split(cleaned_X, encoded_y, test_size=0.2, random_state = 42, stratify = encoded_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK3R-hiumJlF"
      },
      "source": [
        "### ***2.2 Implementing Naive Bayes***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnKcjZgQmJlF"
      },
      "outputs": [],
      "source": [
        "class NaiveBayesClassifier:\n",
        "    \n",
        "    # Initialize the Classifier.\n",
        "    def __init__(self, train_X, train_Y, test_X, test_Y):\n",
        "        self.tokenizer = nltk.tokenize.WhitespaceTokenizer()   # Tokenize possible white spaces in text.\n",
        "        self.train_sentences = train_X\n",
        "        self.train_labels = train_Y\n",
        "        self.test_sentences = test_X\n",
        "        self.test_labels = test_Y\n",
        "        self.priors = {}\n",
        "        self.label_items = {}\n",
        "        self.word_counts = {}\n",
        "        self.vocab = None\n",
        "    \n",
        "    # BoW Method.\n",
        "    def bow(self, ngram, stopword):\n",
        "        vectorizer = CountVectorizer(ngram_range = ngram, min_df = 3, stop_words = stopword)\n",
        "        X = vectorizer.fit_transform(self.train_sentences)\n",
        "        self.vocab = vectorizer.get_feature_names_out()\n",
        "        X = X.toarray()\n",
        "        # Calculate the frequencies of words in descriptions.\n",
        "        for l in range(10):\n",
        "            # Create a dict in dict for each genre type.\n",
        "            self.word_counts[l] = defaultdict(lambda: 0)\n",
        "        # Fill the dicts by word frequencies.\n",
        "        for i in range(X.shape[0]):\n",
        "            l = self.train_labels[i]\n",
        "            for j in range(len(self.vocab)):\n",
        "                self.word_counts[l][self.vocab[j]] += X[i][j]\n",
        "    \n",
        "    # Laplace smoothing.\n",
        "    def laplace_smoothing(self, word, text_label):\n",
        "        # Increase the divident with 1.\n",
        "        a = self.word_counts[text_label][word] + 1\n",
        "        # Increase the divisor with feature count.\n",
        "        b = self.label_items[text_label] + len(self.vocab)\n",
        "        # Calculate log probability.\n",
        "        return math.log(a/b)\n",
        "    \n",
        "    # Fit the training set.\n",
        "    def fit(self, x, y, labels):\n",
        "        # Group data based on labels.\n",
        "        grouped_data = {}\n",
        "        for l in labels:\n",
        "            grouped_data[l] = x[np.where(y == l)]\n",
        "        # Arrange priors.\n",
        "        for l, data in grouped_data.items():\n",
        "            self.label_items[l] = len(data)\n",
        "            self.priors[l] = math.log(self.label_items[l] / len(x))\n",
        "    \n",
        "    # Predict the genre.\n",
        "    def predict(self, labels, x):\n",
        "        result = []\n",
        "        for text in x:\n",
        "            # Arrange label scores according to priors.\n",
        "            label_scores = {l: self.priors[l] for l in labels}\n",
        "            words = set(self.tokenizer.tokenize(text))\n",
        "            # Calculate each label score based on priors. \n",
        "            for word in words:\n",
        "                if word not in self.vocab: \n",
        "                    continue\n",
        "                for l in labels:\n",
        "                    label_scores[l] += self.laplace_smoothing(word, l)\n",
        "            # Choose the label which has more score based on priors.\n",
        "            result.append(max(label_scores, key=label_scores.get))\n",
        "        return result\n",
        "    \n",
        "    # Run Model, then calculate the accuracy.\n",
        "    def calculate_accuracy(self):\n",
        "        labels = [0,1,2,3,4,5,6,7,8,9]\n",
        "        self.fit(self.train_sentences, self.train_labels, labels)\n",
        "        pred = self.predict(labels, self.test_sentences)\n",
        "        return accuracy_score(self.test_labels, pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOPOXJc_mJlF"
      },
      "outputs": [],
      "source": [
        "# Configure Classifiers for Each Experiment. \n",
        "naive_bayes_stopword_uni = NaiveBayesClassifier(X_train, y_train, X_test, y_test)\n",
        "naive_bayes_no_stopword_uni = NaiveBayesClassifier(cleaned_X_train, cleaned_y_train, cleaned_X_test, cleaned_y_test)\n",
        "naive_bayes_stopword_bi = NaiveBayesClassifier(X_train, y_train, X_test, y_test)\n",
        "naive_bayes_no_stopword_bi = NaiveBayesClassifier(cleaned_X_train, cleaned_y_train, cleaned_X_test, cleaned_y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5sSziBemJlF"
      },
      "outputs": [],
      "source": [
        "# Unigram with Stopwords Experiment.\n",
        "naive_bayes_stopword_uni.bow((1,1), None)\n",
        "stopword_uni = naive_bayes_stopword_uni.calculate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSJN-229mJlF"
      },
      "outputs": [],
      "source": [
        "# Unigram without Stopwords Experiment\n",
        "naive_bayes_no_stopword_uni.bow((1,1), 'english')\n",
        "no_stopword_uni = naive_bayes_no_stopword_uni.calculate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "DZQmr9v-mJlG"
      },
      "outputs": [],
      "source": [
        "# Bigram with Stopwords Experiment.\n",
        "naive_bayes_stopword_bi.bow((2,2), None)\n",
        "stopword_bi = naive_bayes_stopword_bi.calculate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3NkVg6ZmJlG"
      },
      "outputs": [],
      "source": [
        "# Bigram without Stopwords Experiment.\n",
        "naive_bayes_no_stopword_bi.bow((2,2), 'english')\n",
        "no_stopword_bi = naive_bayes_no_stopword_bi.calculate_accuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcOG9b6xmJlG"
      },
      "source": [
        "### ***2.3 Error Analysis***\n",
        "\n",
        "#### There can be mainly 4 problem in Naive Bayes Classification when misestimation occurred:\n",
        "##### 1. Feature Independency\n",
        "***We assume that all features are being independent of each other but when the problem turns into a real-life scenario, this assumption rarely holds true.***\n",
        "##### 2. Zero-Frequency Problem\n",
        "***Frequency-based probability estimate will be zero, when an individual class label is missing.***\n",
        "##### 3. Data Scarity\n",
        "***Insufficient data may lead to the algorithm to numerical instabilities resulting in vague prediction by the classifier model.***\n",
        "##### 4. Poorly Preprocessed Data\n",
        "***Real world data is raw, so when a developing Naive Bayes model we have to preprocess the data.***\n",
        "\n",
        "#### Analysis of the misestimation based on these problems:\n",
        "* ***Zero-Frequency problem prevented by applying laplace-smoothing.***\n",
        "* ***Feature Independency may cause of these misestimations. Some books may be belong to 2 or more category at the same time, so these books descriptions can contain words from multiple categories.***\n",
        "* ***Distribution of genre types are not equal, so data scarity may occurs the misestimation problem.***\n",
        "* ***Maybe I can't clean and preprocess defects in the description data well enough, so model doesn't fit well. Because of that reason, some of the book are misclassified.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjncO7A0mJlG"
      },
      "source": [
        "### ***2.4 Modul Analysis***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U66cVOyRmJlG"
      },
      "source": [
        "#### ***2.4.a  Analyzing effect of the words on prediction***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmb5eiUUmJlG"
      },
      "outputs": [],
      "source": [
        "def show_presence_absence(genre_type):\n",
        "    \n",
        "    # Group the genres by specified genre.\n",
        "    genre_type_books = df.groupby(['genre']).get_group(genre_type)\n",
        "    \n",
        "    # Take all descriptions of that genre.\n",
        "    descriptions = genre_type_books.cleaned_description.values\n",
        "    \n",
        "    # Create a TF-IDF Vectorizer for calculate weights based on conditional probability.\n",
        "    vec = TfidfVectorizer()\n",
        "    \n",
        "    # Calculate weight of each word in the specified genre.\n",
        "    tf_idf =  vec.fit_transform(descriptions)\n",
        "    \n",
        "    # Calculate top 10 Presence and top 10 Absence Words.\n",
        "    pa_df = pd.DataFrame(tf_idf.toarray(), columns=vec.get_feature_names_out())\n",
        "    sum_df = pa_df.sum().sort_values(ascending = False)\n",
        "    print(f\"Top 10 Presence Word of {genre_type}\")\n",
        "    display(sum_df[:10])\n",
        "    print(f\"\\nTop 10 Absence Word of {genre_type}\")\n",
        "    display(sum_df[-10:])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "X3jevYBImJlG",
        "outputId": "1484de19-5800-4432-dfdd-8d2852900573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 Presence Word of Young Adult\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "life       58.282313\n",
              "new        54.833681\n",
              "love       48.075244\n",
              "school     46.174400\n",
              "world      44.724624\n",
              "just       39.510522\n",
              "girl       38.606984\n",
              "year       37.814978\n",
              "friends    36.046324\n",
              "time       35.885485\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Absence Word of Young Adult\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "frostbiterose       0.047297\n",
              "academylissa        0.047297\n",
              "kissrose            0.047297\n",
              "stonehouse          0.044589\n",
              "theworstbookever    0.044589\n",
              "zandri              0.044589\n",
              "boiled              0.044589\n",
              "aaronpatterson      0.044589\n",
              "mstersmith          0.044589\n",
              "blogspot            0.044589\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Presence Word of Fantasy\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "world     95.110394\n",
              "new       76.753067\n",
              "magic     69.048592\n",
              "life      68.367681\n",
              "time      62.165881\n",
              "power     49.386563\n",
              "king      48.944918\n",
              "book      48.828420\n",
              "series    47.763852\n",
              "dark      47.474750\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Absence Word of Fantasy\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ripercussioni    0.042019\n",
              "nei              0.042019\n",
              "solcano          0.042019\n",
              "mai              0.042019\n",
              "porta            0.042019\n",
              "solitudine       0.042019\n",
              "eppure           0.042019\n",
              "divora           0.042019\n",
              "sollecita        0.042019\n",
              "avere            0.042019\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Presence Word of Classics\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "novel      16.551707\n",
              "story      16.163125\n",
              "life       15.137923\n",
              "new        14.187331\n",
              "love       13.164633\n",
              "world      12.817884\n",
              "young      11.592062\n",
              "edition    11.344683\n",
              "stories    11.160772\n",
              "tale       10.762718\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Absence Word of Classics\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "gale         0.027459\n",
              "irons        0.027459\n",
              "succdait     0.027459\n",
              "donner       0.027459\n",
              "rappel       0.027459\n",
              "accepter     0.027459\n",
              "populaire    0.027459\n",
              "fort         0.027459\n",
              "jusqu        0.027459\n",
              "retourn      0.027459\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Presence Word of Science Fiction\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "world      25.221669\n",
              "earth      24.477492\n",
              "planet     21.754202\n",
              "new        20.598337\n",
              "human      19.730130\n",
              "life       18.512623\n",
              "time       18.274109\n",
              "war        16.717374\n",
              "science    16.252629\n",
              "space      15.841428\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Absence Word of Science Fiction\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "galley           0.025254\n",
              "unintentional    0.025254\n",
              "astray           0.025254\n",
              "evitable         0.025254\n",
              "verse            0.025254\n",
              "satisfaction     0.025254\n",
              "robbie           0.025254\n",
              "tercentenary     0.025254\n",
              "runaround        0.025254\n",
              "liar             0.025254\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Presence Word of Fiction\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "life      97.400085\n",
              "novel     86.619769\n",
              "world     79.237466\n",
              "love      78.531064\n",
              "new       77.600116\n",
              "story     77.086735\n",
              "family    68.990492\n",
              "man       60.956649\n",
              "young     57.657292\n",
              "time      51.478449\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Absence Word of Fiction\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "uzis             0.030141\n",
              "fiefdoms         0.030141\n",
              "vulgarities      0.030141\n",
              "loudmouths       0.030141\n",
              "cockneyisms      0.030141\n",
              "rollers          0.030141\n",
              "flirtatiously    0.030141\n",
              "ruminate         0.030141\n",
              "insuring         0.030141\n",
              "plagiarize       0.023797\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Presence Word of Horror\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "new       13.516661\n",
              "world     12.644026\n",
              "horror    11.733001\n",
              "house     11.451890\n",
              "life      11.007154\n",
              "story     10.418000\n",
              "town      10.382720\n",
              "man       10.359287\n",
              "old        9.900066\n",
              "dead       9.856169\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Absence Word of Horror\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "macdonald       0.045304\n",
              "ledge           0.045304\n",
              "ladder          0.045304\n",
              "battleground    0.045304\n",
              "fll             0.045304\n",
              "quitters        0.045304\n",
              "hellish         0.045304\n",
              "rung            0.045304\n",
              "fw              0.045304\n",
              "gallery         0.045304\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Presence Word of Romance\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "love      72.277248\n",
              "life      68.586490\n",
              "new       48.624354\n",
              "man       47.789836\n",
              "just      42.137954\n",
              "heart     38.180313\n",
              "time      37.025938\n",
              "woman     36.963350\n",
              "family    34.950529\n",
              "like      34.718493\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Absence Word of Romance\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "endangering    0.02682\n",
              "pledging       0.02682\n",
              "unanimous      0.02682\n",
              "thieu          0.02682\n",
              "whistles       0.02682\n",
              "obliging       0.02682\n",
              "revelry        0.02682\n",
              "gored          0.02682\n",
              "waif           0.02682\n",
              "paine          0.02682\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Presence Word of Mystery\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "new          30.093188\n",
              "murder       27.928576\n",
              "killer       26.973581\n",
              "life         24.037760\n",
              "case         22.937338\n",
              "mystery      22.863099\n",
              "old          22.738117\n",
              "man          22.265749\n",
              "detective    21.763649\n",
              "death        20.549863\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Absence Word of Mystery\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "indebted         0.045595\n",
              "rewards          0.045595\n",
              "hefty            0.045595\n",
              "rescuing         0.045595\n",
              "conflagration    0.045595\n",
              "tangent          0.045595\n",
              "wooed            0.045595\n",
              "valued           0.045595\n",
              "psycho           0.045595\n",
              "utilize          0.045595\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Presence Word of History\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "history     23.059677\n",
              "war         21.473389\n",
              "world       17.615884\n",
              "new         14.282164\n",
              "book        13.328499\n",
              "story       13.159550\n",
              "american    12.334931\n",
              "life        11.448896\n",
              "years       10.210757\n",
              "great        9.691610\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Absence Word of History\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "cow              0.032862\n",
              "giornale         0.032862\n",
              "tojo             0.032862\n",
              "togo             0.032862\n",
              "choosing         0.032862\n",
              "isolationists    0.032862\n",
              "evoke            0.032862\n",
              "dictatorial      0.032862\n",
              "springfield      0.032862\n",
              "consolidated     0.032862\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Presence Word of Thriller\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "life        8.723201\n",
              "new         8.059924\n",
              "world       7.685948\n",
              "man         7.427947\n",
              "reacher     7.382393\n",
              "killer      6.371205\n",
              "thriller    6.075177\n",
              "time        5.813557\n",
              "woman       5.774562\n",
              "secret      5.713321\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 Absence Word of Thriller\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "glowing        0.031804\n",
              "luminous       0.031804\n",
              "breather       0.031804\n",
              "storylines     0.031804\n",
              "religions      0.031804\n",
              "instance       0.031804\n",
              "charlemagne    0.031804\n",
              "manic          0.031804\n",
              "turners        0.031804\n",
              "essentially    0.031804\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Show top 10 presence and absence words for each genre.\n",
        "for genre in df.genre.unique():\n",
        "    show_presence_absence(genre)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZiPVvramJlG"
      },
      "outputs": [],
      "source": [
        "class NaiveBayesClassifierUpdated:\n",
        "    \n",
        "    # Initialize the Classifier.\n",
        "    def __init__(self, train_X, train_Y, test_X, test_Y):\n",
        "        self.tokenizer = nltk.tokenize.WhitespaceTokenizer()   # Tokenize possible white spaces in text.\n",
        "        self.train_sentences = train_X\n",
        "        self.train_labels = train_Y\n",
        "        self.test_sentences = test_X\n",
        "        self.test_labels = test_Y\n",
        "        self.priors = {}\n",
        "        self.label_items = {}\n",
        "        self.word_counts = {}\n",
        "        self.vocab = None\n",
        "    \n",
        "    # TF-IDF Method.\n",
        "    def tf_idf(self, stopword):\n",
        "        vectorizer = TfidfVectorizer(ngram_range = (1,1), stop_words = stopword)\n",
        "        X = vectorizer.fit_transform(self.train_sentences)\n",
        "        self.vocab = vectorizer.get_feature_names_out()\n",
        "        X = X.toarray()\n",
        "        # Calculate the frequencies of words in descriptions.\n",
        "        for l in range(10):\n",
        "            self.word_counts[l] = defaultdict(lambda: 0)\n",
        "        for i in range(X.shape[0]):\n",
        "            l = self.train_labels[i]\n",
        "            for j in range(len(self.vocab)):\n",
        "                self.word_counts[l][self.vocab[j]] += X[i][j]\n",
        "    \n",
        "    # Laplace smoothing\n",
        "    def laplace_smoothing(self, word, text_label):\n",
        "        # Increase the divident with 1.\n",
        "        a = self.word_counts[text_label][word] + 1\n",
        "        # Increase the divisor with feature count.\n",
        "        b = self.label_items[text_label] + len(self.vocab)\n",
        "        # Calculate log probability.\n",
        "        return math.log(a/b)\n",
        "    \n",
        "    # Fit the training set.\n",
        "    def fit(self, x, y, labels):\n",
        "        # Group data based on labels.\n",
        "        grouped_data = {}\n",
        "        for l in labels:\n",
        "            grouped_data[l] = x[np.where(y == l)]\n",
        "        # Arrange priors.\n",
        "        for l, data in grouped_data.items():\n",
        "            self.label_items[l] = len(data)\n",
        "            self.priors[l] = math.log(self.label_items[l] / len(x))\n",
        "    \n",
        "    # Predict the genre.\n",
        "    def predict(self, labels, x):\n",
        "        result = []\n",
        "        for text in x:\n",
        "            # Arrange label scores according to priors.\n",
        "            label_scores = {l: self.priors[l] for l in labels}\n",
        "            words = set(self.tokenizer.tokenize(text))\n",
        "            # Calculate each label score based on priors. \n",
        "            for word in words:\n",
        "                if word not in self.vocab: \n",
        "                    continue\n",
        "                for l in labels:\n",
        "                    label_scores[l] += self.laplace_smoothing(word, l)\n",
        "            # Choose the label which has more score based on priors.\n",
        "            result.append(max(label_scores, key=label_scores.get))\n",
        "        return result\n",
        "    \n",
        "    # Run Model, then calculate the accuracy.\n",
        "    def calculate_accuracy(self):\n",
        "        labels = [0,1,2,3,4,5,6,7,8,9]\n",
        "        self.fit(self.train_sentences, self.train_labels, labels)\n",
        "        pred = self.predict(labels, self.test_sentences)\n",
        "        return accuracy_score(self.test_labels, pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDD5XlywmJlH"
      },
      "outputs": [],
      "source": [
        "tfidf_stopword_uni = NaiveBayesClassifierUpdated(X_train, y_train, X_test, y_test)\n",
        "tfidf_stopword_uni.tf_idf(None)\n",
        "stopword_tfidf = tfidf_stopword_uni.calculate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "js3jl6j9mJlH"
      },
      "outputs": [],
      "source": [
        "tfidf_no_stopword_uni = NaiveBayesClassifierUpdated(cleaned_X_train, cleaned_y_train, cleaned_X_test, cleaned_y_test)\n",
        "tfidf_no_stopword_uni.tf_idf('english')\n",
        "no_stopword_tfidf = tfidf_no_stopword_uni.calculate_accuracy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vZG0F4jmJlH"
      },
      "source": [
        "#### ***2.4.b Stop Words***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE_aDDNGmJlH",
        "outputId": "93b4fea7-4861-413d-9f90-c84419a9b864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 non-stop words that most strongly predict that the Fantasy books\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>world</td>\n",
              "      <td>2779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>new</td>\n",
              "      <td>1991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>life</td>\n",
              "      <td>1744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>magic</td>\n",
              "      <td>1544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>time</td>\n",
              "      <td>1373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>power</td>\n",
              "      <td>1010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>old</td>\n",
              "      <td>972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>book</td>\n",
              "      <td>971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>dark</td>\n",
              "      <td>955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>love</td>\n",
              "      <td>942</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Word  Frequency\n",
              "0  world       2779\n",
              "1    new       1991\n",
              "2   life       1744\n",
              "3  magic       1544\n",
              "4   time       1373\n",
              "5  power       1010\n",
              "6    old        972\n",
              "7   book        971\n",
              "8   dark        955\n",
              "9   love        942"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Group the genres, then choose Fantasy genre.\n",
        "fantasy_books = df.groupby(['genre']).get_group('Fantasy')\n",
        "\n",
        "# Calculate the word frequencies in Fantasy genre.\n",
        "fantasy_books_df = fantasy_books.cleaned_description.str.split(expand=True).stack().value_counts().reset_index()\n",
        "\n",
        "# Arrange column names. \n",
        "fantasy_books_df.columns = ['Word', 'Frequency'] \n",
        "\n",
        "# Display most frequent 10 words in History genre. \n",
        "print(f\"10 non-stop words that most strongly predict that the Fantasy books\")\n",
        "fantasy_books_df.iloc[:10, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4l2TCEomJlH",
        "outputId": "830f9d28-a545-4e87-e355-e0f73e1ab5a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 non-stop words that most strongly predict that the Mystery books\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word</th>\n",
              "      <th>Frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>new</td>\n",
              "      <td>797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>murder</td>\n",
              "      <td>632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>life</td>\n",
              "      <td>581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>killer</td>\n",
              "      <td>570</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>case</td>\n",
              "      <td>499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>man</td>\n",
              "      <td>492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>mystery</td>\n",
              "      <td>482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>old</td>\n",
              "      <td>476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>detective</td>\n",
              "      <td>441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>family</td>\n",
              "      <td>419</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Word  Frequency\n",
              "0        new        797\n",
              "1     murder        632\n",
              "2       life        581\n",
              "3     killer        570\n",
              "4       case        499\n",
              "5        man        492\n",
              "6    mystery        482\n",
              "7        old        476\n",
              "8  detective        441\n",
              "9     family        419"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Group the genres, then choose Mystery genre.\n",
        "mystery_books = df.groupby(['genre']).get_group('Mystery')\n",
        "\n",
        "# Calculate the word frequencies in Mystery genre.\n",
        "mystery_books_df = mystery_books.cleaned_description.str.split(expand=True).stack().value_counts().reset_index()\n",
        "\n",
        "# Arrange column names. \n",
        "mystery_books_df.columns = ['Word', 'Frequency'] \n",
        "\n",
        "# Display most frequent 10 words in Mystery genre. \n",
        "print(\"10 non-stop words that most strongly predict that the Mystery books\")\n",
        "mystery_books_df.iloc[:10, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xFOEy0TmJlH"
      },
      "source": [
        "#### ***2.4.c Analyzing effect of the stop words***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WTpzoZkmJlH"
      },
      "source": [
        "###### ***Removing stop words***\n",
        "***The main point of the removing stopwords is \"don’t add any new information\". Based on this principle, when we remove the stopwords the data which we will investigates decreases so it makes the algorithm faster and decreases the time complexity. Also, most of the stopwords are general and frequent in the different train records; so, in the BoW method that situation can't make a difference on different classified records.***\n",
        "\n",
        "###### ***Holding stop words***\n",
        "***Stopwords can change the positivity or negativity of a word, so problems like sentiment analysis can easily affect from the stopwords. When non-stopwords gain a mean with the stopwords, we have to hold stopwords. But holding stopwords want more memory than removing stop words, so algorithm be more memory inefficient.***\n",
        "\n",
        "###### ***Interpretation over Problem***\n",
        "***Stopwords will not affect our problem because words which we need mostly doesn't depend on negativity or positivity. Probably, algorithm will take more memory and time complexity for the stopwords included experiments but give closer accuracy with not stopwords included experiments. As a result, dropping stopwords from the data will be more efficient.*** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wx8G2P1mJlH"
      },
      "source": [
        "### ***2.5 Calculation of Accuracy***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGmkzC2emJlH",
        "outputId": "980ca8f5-ed69-4be1-f0cf-d0d01415e90a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Unigram with Stopwords Experiment: %47.13\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy of Unigram with Stopwords Experiment: %{100 * stopword_uni:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPkmUWtzmJlI",
        "outputId": "ebc307cc-de68-4552-896c-ecccd3350f3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Unigram without Stopwords Experiment: %48.51\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy of Unigram without Stopwords Experiment: %{100 * no_stopword_uni:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU-DxfbKmJlI",
        "outputId": "4b6915c0-46b3-46d6-bc62-6cd8a38df6d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Bigram with Stopwords Experiment: %23.03\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy of Bigram with Stopwords Experiment: %{100 * stopword_bi:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGs3YUJFmJlI",
        "outputId": "e8e161f8-4f36-48e3-dd76-6c83af9908a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of Bigram without Stopwords Experiment: %23.03\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy of Bigram without Stopwords Experiment: %{100 * no_stopword_bi:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enZgtTA4mJlI",
        "outputId": "e5ed20f1-d74b-4237-fb18-95876ea86f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuract of TF-IDF with Stopwords Experiment: %44.05\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuract of TF-IDF with Stopwords Experiment: %{100 * stopword_tfidf:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNemC3bcmJlI",
        "outputId": "b9e616d1-2b47-4998-890e-53ef0b5bc0cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuract of TF-IDF without Stopwords Experiment: %48.48\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuract of TF-IDF without Stopwords Experiment: %{100 * no_stopword_tfidf:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5mZRsxPxIUX"
      },
      "source": [
        "# ***REPORT***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfeZb0WzxMcz"
      },
      "source": [
        "### 1-) Overview of the Problem\n",
        "\n",
        "#### In this problem, we are trying to develop a Naive Bayes Algorithm from scratch with BoW and TF-IDF methods for text classification.  The subject of the text classification is description and book genre pairs, we are predicting the book's genre with using it's description.\n",
        "\n",
        "### 2-) Preprocessing the Data\n",
        "\n",
        "#### In this part; first I drop the author and title columns, remove punctuations and numbers, convert all descriptions to lowercase, remove the non-english words like arabic words. After that process; I remove stopwords from the descriptions and then add this cleaned descriptions to dataframe as a new column because we will observe our Naive Bayes model on both of the descriptions which includes stopwords or not. After this new column creation; I check tabs, newlines and single chars for both of the cleaned and not-cleaned description columns. \n",
        "\n",
        "### 3-) Understanding the Data Part\n",
        "\n",
        "#### In this part; I group the dataframe according to genre types, then choose History genre. I count the word frequencies for the History genre and show the first 3 records which frequencies are highest. According to results; history genre can guess by using \"history\", \"war\" and \"world\" words. That shows each genre has unique and frequent words which can describe the type of the genre.\n",
        "\n",
        "* #### Split the Dataset\n",
        "    * #### In this part I split the features and labels. I use two feature space because I will test my algorithm for both of the descriptions which contains stopwords or not. I split my dataset to %80 Train and %20 test, and doing this process with randomized.\n",
        "\n",
        "### 4-) Implementing Naive Bayes Part\n",
        "\n",
        "#### In this part; I implement my own Naive Bayes Class based on the BoW method. The class contains initializer, bow, laplace_smoothing, fit, predict and calculate accuracy methods. \n",
        "\n",
        "##### Interpreting of these methods:\n",
        "* #### init(): initializes the dictionaries, variables, sets which I will use in the class.\n",
        "* #### bow(): makes the bow method, I use 3 parameter for this method. First parameter is ngram_range which defines the count of word in sample, second parameter is min_df which removes the words frequency dictionary which word has lower occurrences than threshold value, the last parameter is stop_words which removes stopwords.\n",
        "##### Explanation of why I use min_df = 3 ?\n",
        "##### The bi_gram experiment gave a RAM Issues, to prevent from this issue I had 2 ways which are take a sample or decrease the dictionary size. If I took a sample, some of the genres will have too few occurrences in my train set it will cause problems on my prediction step so I choose the decrease the dictionary size with using min_df = 3 parametes which drops the words whose occurrences less than 3.\n",
        "* #### laplace_smoothing(): apply laplace smoothing to prevent zero-frequency problem, then calculate log probability.\n",
        "* #### fit(): fits the model based on train sets.\n",
        "* #### predict(): predicts the genres of the book by given descriptions.\n",
        "* #### calculate_accuracy(): run the model, then calculate the accuracy of the model.\n",
        "\n",
        "#### I create 4 classifier for use in experiments after implemented the Naive Bayes. I tested my Naive Bayes with BoW method on:\n",
        "* #### Include Stopword - Unigram\n",
        "* #### Not Include Stopword - Unigram\n",
        "* #### Include Stopword - Bigram\n",
        "* #### Not Include Stopword - Bigram\n",
        "\n",
        "### 5-) Error Analysis Part\n",
        "\n",
        "#### I analyzed misestimation reasons based on naive bayes issues, and interpret them under the related section.\n",
        "\n",
        "### 6-) Modul Analysis Part\n",
        "\n",
        "* #### Analyzing effect of the words on prediction\n",
        "    * #### I show the top 10 most absent and presence words of each genre type.\n",
        "    * #### Reimplement the Naive Bayes based on TF-IDF methodology. I use TF-IDFVectorizer which does same operations with TF-IDFTransformer but works faster than it. After new implementation, I create 2 classifier to test my Naive Bayes with TF-IDF method on:\n",
        "        * #### Include Stopword\n",
        "        * #### Not-Include Stopword\n",
        "    * #### In the absence or presence of the word we consider the conditional probability of the word in a genre type. The condition includes the word occurrences in the all words, which has more frequency be presence, low frequency be absence. Precence words are more important because they are mostly unique for the specified genre type and seperate the genre from the others. Absence words can think as stopwords so we can drop them because they are not unique too much and has no weight while classifying the book genre.\n",
        "\n",
        "* #### Stop Words and Analyzing Effect of the Stopwords\n",
        "    * #### I show top 10 non-stop words for the Fantasy and Mysters genres, then analyze the effects of the stopwords in the related part.\n",
        "\n",
        "### 7-) Result Analysis\n",
        "\n",
        "![Question1](https://drive.google.com/uc?id=17489OWskvEO2pqcl2wo4LHw1bFNoQgc_)\n",
        "\n",
        "1. #### Comment on BoW Method - Unigram Experiments\n",
        "\n",
        "#### As expected, the removing stopwords increases the acccuracy. Because we take descriptions word by word and the result just affected from the individual words.\n",
        "\n",
        "2. #### Comment on BoW Method - Bigram Experiments\n",
        "\n",
        "#### The result is not affected from the removing stopwords and the accuracy decreases too much relative to unigram. There are several reasons for these accuracies. Possible reasons are:\n",
        "* #### Data Preprocessing Step: Maybe I can't preprocess the data correctly for bi_gram experiment. In bi_gram experiment the words take as 2 long pairs such as \"life long\", because of the not convenient preprocess steps my classifier not works well on bi_gram range.\n",
        "* #### Splitting the Data Randomly: I use random variable to split data, because of these reason some of the genre types are not occur in the train data so bi_gram can't take much word for that genre so that genre books can misclassified. That situation can easily decrease the accuracy.\n",
        "* #### Last reason can be taking 2 words as pair maybe occur meaningless combination like \"flower hell\" and if we have not enough frequency for these meaningless pairs we can't fit the model well so accuracy decreases.\n",
        "\n",
        "***Our results show there is no difference for included stopword - not included stopword situation, I think the word pairs can't occur any difference from individual pairs and that shows either our preprocessing steps are not appropriate for the bi_gram experiment nor randomly split defects the distribution of genre types in train set.***\n",
        "\n",
        "3. #### Comment on TF-IDF Method Experiments\n",
        "\n",
        "#### The results are changed as expected, removing stopwords are increased the accuracy. As showed in the related part some words have big some words have small weights. TF-IDF works based on this principle. It gives the close accuracy the bow method, that shows tf-idf is not optimize our model better than the bow method. This optimization can depend on preprocess and data splitting, so we can't generalize the tf-idf is not optimize well the naive bayes. That approach just acceptable for my implementation and methodology steps.  \n",
        "\n",
        "### 8-) Conclusion\n",
        "#### In this project I implement Naive Bayes with BoW and TF-IDF methods to predict book genres depend on descriptions of books. I observed 6 experiment which are \n",
        "* (BoW, Stopword, Unigram)\n",
        "* (BoW, No Stopword, Unigram)\n",
        "* (BoW, Stopword, Bigram)\n",
        "* (BoW, No Stopword, Bigram)\n",
        "* (TF-IDF, Stopword)\n",
        "* (TF-IDF, No Stopword).\n",
        "\n",
        "#### Based on these experiements, as a result; depending on my preprocessing, random splitting and Naive Bayes implementation model works better on unigram range without stopwords. That shows Naive Bayes can be convenient classifier for text classification. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}